{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import src.VAE_LSTM_CNN as vae\n",
    "\n",
    "onBody = pd.read_pickle('dataset/onBody.pkl')\n",
    "onBody_val = pd.read_pickle('dataset/onBody_Val.pkl')\n",
    "anomoly = pd.read_pickle('dataset/offBody.pkl')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SAMPLE_CHOPPED = 2000\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, normal_df, anomaly_df):\n",
    "        self.normal_samples = normal_df\n",
    "        self.anomaly_samples = anomaly_df\n",
    "\n",
    "        self.class_labels = normal_df['dvc'].unique()\n",
    "\n",
    "        self.data_by_class = {}\n",
    "        for class_label in self.class_labels:\n",
    "        # Filter samples by class and store them\n",
    "            class_samples = normal_df[normal_df['dvc'] == class_label]\n",
    "            self.data_by_class[class_label] = np.array(class_samples['freq_dev'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.anchor_indices = []\n",
    "        for class_label, samples in self.data_by_class.items():\n",
    "            n = len(samples)\n",
    "            self.anchor_indices.extend([(class_label, i) for i in range(n)])\n",
    "\n",
    "        # # Split the normal samples into two halves for anchors and positives\n",
    "        # x = train_test_split(self.normal_samples, test_size=0.5, random_state=42)\n",
    "        # self.anchor_samples =  x[0].reset_index(drop=True)\n",
    "        # self.positive_samples = x[1].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # The dataset length will be the number of normal samples divided by 2, \n",
    "        # since we're using half for anchors and half for positives\n",
    "        return len(self.anchor_indices)\n",
    "        # return len(self.anchor_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_label, anchor_idx = self.anchor_indices[idx]\n",
    "        n = len(self.data_by_class[class_label]) \n",
    "        anchor = self.data_by_class[class_label][anchor_idx]\n",
    "        positive_idx = (anchor_idx + np.random.randint(1, n)) % n \n",
    "        positive = self.data_by_class[class_label][positive_idx]\n",
    "        \n",
    "\n",
    "        # choose the other class_labels randomly\n",
    "        other_class_label = class_label\n",
    "        while other_class_label == class_label:\n",
    "            other_class_label = self.class_labels[np.random.randint(len(self.class_labels))]\n",
    "        \n",
    "        # Randomly select a negative sample from the other class\n",
    "        negative1 = self.data_by_class[other_class_label][np.random.randint(len(self.data_by_class[other_class_label]))]\n",
    "        \n",
    "        # Randomly select a negative sample from the anomaly samples\n",
    "        negative2 = self.anomaly_samples[np.random.randint(len(self.anomaly_samples))]\n",
    "\n",
    "        #randomly select the negative between negative1 and negative2\n",
    "        negative = negative1 if np.random.random() > 0.5 else negative2\n",
    "\n",
    "        # negative = negative2\n",
    "\n",
    "        # anchor = self.anchor_samples.iloc[idx]['freq_dev']\n",
    "        # positive = self.positive_samples.iloc[idx]['freq_dev']\n",
    "        # negative = self.anomaly_samples[np.random.randint(len(self.anomaly_samples))]\n",
    "\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        anchor = torch.tensor(anchor[1500:1700], dtype=torch.float).float().unsqueeze(0)\n",
    "        positive = torch.tensor(positive[1500:1700], dtype=torch.float).float().unsqueeze(0)\n",
    "        negative = torch.tensor(negative[1500:1700], dtype=torch.float).unsqueeze(0)\n",
    "        \n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up the GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "# Assuming model is your neural network for embedding\n",
    "batch_size = 16\n",
    "margin  = 1\n",
    "\n",
    "latent_dim = 3    # Latent space dimension\n",
    "\n",
    "\n",
    "triplet_dataset = TripletDataset(onBody, anomoly)\n",
    "triplet_dataloader = DataLoader(triplet_dataset, batch_size=batch_size, shuffle=True)\n",
    "#validation\n",
    "triplet_dataset_val = TripletDataset(onBody_val, anomoly)\n",
    "triplet_dataloader_val = DataLoader(triplet_dataset_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "triplet_dataloader_plot = DataLoader(triplet_dataset, batch_size=1, shuffle=True)\n",
    "triplet_dataloader_val_plot = DataLoader(triplet_dataset_val, batch_size=1, shuffle=True)\n",
    "\n",
    "loss_function = TripletLoss(margin =margin).to(DEVICE) \n",
    "\n",
    "\n",
    "ONBODY_model = vae.CNNLSTMEmbeddingNet(input_length=200, num_channels=1 ,embedding_dim=latent_dim).to(DEVICE)\n",
    "optimizer = optim.Adam(ONBODY_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6098, 0.6313, 0.6512,  ..., 0.5715, 0.5852, 0.5982]],\n",
      "\n",
      "        [[0.7751, 0.7675, 0.7567,  ..., 0.8413, 0.8278, 0.8108]],\n",
      "\n",
      "        [[0.6741, 0.6628, 0.6474,  ..., 0.4722, 0.4672, 0.4633]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.7789, 0.7927, 0.8070,  ..., 0.7960, 0.8066, 0.8171]],\n",
      "\n",
      "        [[0.8392, 0.8394, 0.8397,  ..., 0.8565, 0.8555, 0.8540]],\n",
      "\n",
      "        [[0.6049, 0.6203, 0.6354,  ..., 0.5457, 0.5620, 0.5781]]],\n",
      "       device='cuda:0') tensor([[[0.6281, 0.6122, 0.5954,  ..., 0.6602, 0.6446, 0.6291]],\n",
      "\n",
      "        [[0.6271, 0.6220, 0.6182,  ..., 0.7364, 0.7358, 0.7349]],\n",
      "\n",
      "        [[0.8787, 0.8797, 0.8800,  ..., 0.9118, 0.8990, 0.8823]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1754, 0.1612, 0.1506,  ..., 0.2371, 0.2578, 0.2776]],\n",
      "\n",
      "        [[0.0731, 0.0666, 0.0602,  ..., 0.1477, 0.1384, 0.1263]],\n",
      "\n",
      "        [[0.1321, 0.1058, 0.0819,  ..., 0.6506, 0.6397, 0.6283]]],\n",
      "       device='cuda:0') tensor([[[0.6807, 0.6604, 0.6412,  ..., 0.5806, 0.5754, 0.5746]],\n",
      "\n",
      "        [[0.5771, 0.5954, 0.6125,  ..., 0.4933, 0.5115, 0.5302]],\n",
      "\n",
      "        [[0.4286, 0.4169, 0.4065,  ..., 0.5053, 0.4913, 0.4776]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5882, 0.5889, 0.5872,  ..., 0.6018, 0.5966, 0.5898]],\n",
      "\n",
      "        [[0.7877, 0.8085, 0.8311,  ..., 0.9034, 0.9097, 0.9137]],\n",
      "\n",
      "        [[0.2467, 0.2318, 0.2169,  ..., 0.2859, 0.2748, 0.2639]]],\n",
      "       device='cuda:0')\n",
      "None None None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m negative_embed \u001b[38;5;241m=\u001b[39m ONBODY_model(negative\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m200\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(anchor_embed, positive_embed, negative_embed)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_embed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mTripletLoss.forward\u001b[0;34m(self, anchor, positive, negative)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, anchor, positive, negative):\n\u001b[0;32m----> 7\u001b[0m     distance_positive \u001b[38;5;241m=\u001b[39m (\u001b[43manchor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m     distance_negative \u001b[38;5;241m=\u001b[39m (anchor \u001b[38;5;241m-\u001b[39m negative)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m     losses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(distance_positive \u001b[38;5;241m-\u001b[39m distance_negative \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for anchor, positive, negative in triplet_dataloader:\n",
    "        anchor, positive, negative = anchor.to(DEVICE), positive.to(DEVICE), negative.to(DEVICE)\n",
    "        print(anchor, positive, negative)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embed = ONBODY_model(anchor.view(16,200,1))\n",
    "        positive_embed = ONBODY_model(positive.view(16,200,1))\n",
    "        negative_embed = ONBODY_model(negative.view(16,200,1))\n",
    "        print(anchor_embed, positive_embed, negative_embed)\n",
    "        loss = loss_function(anchor_embed, positive_embed, negative_embed)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # scheduler.step()\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for anchor, positive, negative in triplet_dataloader_val:\n",
    "            anchor, positive, negative = anchor.to(DEVICE), positive.to(DEVICE), negative.to(DEVICE)\n",
    "            anchor_embed = ONBODY_model(anchor.view(16,200,1))\n",
    "            positive_embed = ONBODY_model(positive.view(16,200,1))\n",
    "            negative_embed = ONBODY_model(negative.view(16,200,1))\n",
    "            val_loss += loss_function(anchor_embed, positive_embed, negative_embed)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss}, Val Loss: {val_loss.item()}\") \n",
    "\n",
    "\n",
    "#save the model\n",
    "torch.save(ONBODY_model.state_dict(), 'Models/TripletLoss.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
